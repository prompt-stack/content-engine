#!/usr/bin/env python3
"""
STEP 2: Extract links from raw HTML files

Input: extraction_TIMESTAMP/raw_html/*.html
Output: extraction_TIMESTAMP/extracted_links.json
        [
          {
            "newsletter_index": 1,
            "newsletter_subject": "...",
            "raw_links": ["url1", "url2", ...],
            "decoded_links": ["url1", "url2", ...],
            "link_count": 45
          }
        ]
"""

import json
import re
import html
from pathlib import Path
from typing import List, Dict
from bs4 import BeautifulSoup


def extract_links_from_html(html_content: str) -> List[str]:
    """
    Extract all href links from HTML and decode entities

    Args:
        html_content: Raw HTML string

    Returns:
        List of decoded URLs
    """
    # Extract all href="..." patterns
    link_pattern = r'href=["\']([^"\']+)["\']'
    raw_links = re.findall(link_pattern, html_content)

    # Decode HTML entities (&amp; ‚Üí &, &quot; ‚Üí ", etc.)
    decoded_links = [html.unescape(link) for link in raw_links]

    return decoded_links


def extract_links_with_context(html_content: str) -> List[Dict]:
    """
    Extract links WITH curator descriptions from HTML

    Args:
        html_content: Raw HTML string

    Returns:
        List of dicts with 'url' and 'curator_description' keys
    """
    soup = BeautifulSoup(html_content, 'html.parser')

    # Button/CTA texts to skip (not meaningful article descriptions)
    SKIP_TEXTS = [
        "TRY NOW", "LEARN MORE", "START SECURE AUTH", "SUBSCRIBE",
        "GET STARTED", "SIGN UP", "VIEW ALL", "READ MORE", "CLICK HERE"
    ]

    enriched_links = []
    seen_urls = set()  # Avoid duplicates

    for a in soup.find_all('a', href=True):
        url = html.unescape(a['href'])
        text = a.get_text(strip=True)

        # Skip duplicate URLs
        if url in seen_urls:
            continue
        seen_urls.add(url)

        # Determine if text is a meaningful description
        # Keep if: >20 chars, not a button text, and has some substance
        has_context = (
            text and
            len(text) > 20 and
            text not in SKIP_TEXTS and
            not text.isupper()  # Skip all-caps (usually buttons)
        )

        enriched_links.append({
            "url": url,
            "curator_description": text if has_context else None
        })

    return enriched_links


def extract_links_from_directory(extraction_dir: Path):
    """
    Step 2: Extract links from all raw HTML files

    Args:
        extraction_dir: Path to extraction_TIMESTAMP directory

    Returns:
        Path to extracted_links.json
    """
    print("="*80)
    print("STEP 2: EXTRACT LINKS FROM HTML")
    print("="*80)
    print(f"\nüìÅ Extraction directory: {extraction_dir.name}\n")

    # Load newsletters metadata
    metadata_file = extraction_dir / "newsletters.json"
    if not metadata_file.exists():
        print(f"‚ùå Error: {metadata_file} not found")
        print("   Run step1_extract_gmail.py first")
        return None

    with open(metadata_file, 'r', encoding='utf-8') as f:
        newsletters_metadata = json.load(f)

    print(f"üìß Found {len(newsletters_metadata)} newsletters\n")

    # Extract links from each newsletter
    raw_html_dir = extraction_dir / "raw_html"
    results = []
    total_links = 0

    for newsletter in newsletters_metadata:
        index = newsletter['index']
        html_file = newsletter['html_file']
        subject = newsletter['subject']
        sender = newsletter['sender']

        print(f"{index:3d}. {subject[:60]}")
        print(f"      From: {sender}")

        # Read raw HTML
        html_path = raw_html_dir / html_file
        if not html_path.exists():
            print(f"      ‚ö†Ô∏è  HTML file not found: {html_file}")
            print()
            continue

        with open(html_path, 'r', encoding='utf-8') as f:
            html_content = f.read()

        # Extract links WITH curator descriptions
        enriched_links = extract_links_with_context(html_content)
        link_count = len(enriched_links)
        total_links += link_count

        # Count how many have meaningful descriptions
        with_context_count = sum(1 for link in enriched_links if link['curator_description'])

        print(f"      Extracted: {link_count} links ({with_context_count} with curator descriptions)")
        print()

        # Store results
        results.append({
            "newsletter_index": index,
            "newsletter_subject": subject,
            "newsletter_sender": sender,
            "newsletter_date": newsletter['date'],
            "html_file": html_file,
            "links": enriched_links,
            "link_count": link_count
        })

    # Save extracted links
    output_file = extraction_dir / "extracted_links.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print("="*80)
    print("STEP 2 COMPLETE")
    print("="*80)
    print(f"\nüìä Summary:")
    print(f"   Newsletters processed: {len(results)}")
    print(f"   Total links extracted: {total_links}")
    print(f"   Average per newsletter: {total_links // len(results) if results else 0}")
    print()
    print(f"üìÇ File created:")
    print(f"   ‚Ä¢ extracted_links.json - All links from all newsletters")
    print()
    print(f"üöÄ Next step:")
    print(f"   python3.11 step3_resolve_redirects.py {extraction_dir.name}")
    print()

    return output_file


if __name__ == '__main__':
    import argparse
    import sys

    parser = argparse.ArgumentParser(
        description='Step 2: Extract links from raw HTML files',
        epilog='''
Examples:
  # Process specific extraction
  python3.11 step2_extract_links.py extraction_20251015_131800

  # Process latest extraction
  python3.11 step2_extract_links.py latest
        ''',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        'extraction_dir',
        help='Extraction directory name (or "latest" for most recent)'
    )

    args = parser.parse_args()

    # Find extraction directory
    output_dir = Path(__file__).parent / "output"

    if args.extraction_dir == 'latest':
        # Find most recent extraction_* directory
        extraction_dirs = sorted(output_dir.glob('extraction_*'), reverse=True)
        if not extraction_dirs:
            print("‚ùå No extraction directories found in output/")
            sys.exit(1)
        extraction_dir = extraction_dirs[0]
    else:
        extraction_dir = output_dir / args.extraction_dir
        if not extraction_dir.exists():
            print(f"‚ùå Directory not found: {extraction_dir}")
            sys.exit(1)

    extract_links_from_directory(extraction_dir)
